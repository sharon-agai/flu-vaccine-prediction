{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flu_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cYtCqiZKaaIF",
        "PobqFeEresO_",
        "dzO3jWHo0SIW",
        "IDL14RGaimox",
        "6Q58L9Tu5cGs",
        "Y9LtvS57aNqz",
        "K9D0V4o7EVYX",
        "DXTtCGMW_akw",
        "Pc-hKbx-EbTp"
      ],
      "authorship_tag": "ABX9TyPV/RoGUPm+yTmtIEddjvLu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j6Jh3frbG64",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "660c15b8-2839-4dcf-b65c-75fc7057369a"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import itertools \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "# from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "% matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYtCqiZKaaIF",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PobqFeEresO_",
        "colab_type": "text"
      },
      "source": [
        "### Basic cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIzFDbfkcrAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# finding null values\n",
        "def nullFinder(df, capitalizedDatasetName):\n",
        "    df_counts = pd.DataFrame(len(df) - df.count())\n",
        "    df_counts.columns = ['# missing values']\n",
        "    display(df_counts.style\n",
        "            .set_caption('{} DATA'.format(capitalizedDatasetName))\n",
        "            .set_table_styles([{'selector': 'caption', \n",
        "                                'props': [('color', 'black'),\n",
        "                                          ('font-size', '16px')]}]) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_UGRGfIdY5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# employment_industry and employment_occupation have several missing values, probably because that information isn't\n",
        "# relevant to people whose employment_status indicates that they're unemployed - MISSING NOT AT RANDOM\n",
        "# also drop health insurance, because half of its values are missing\n",
        "def organize(df):\n",
        "    df.loc[df['employment_status']=='Unemployed', 'employment_industry'] = 'NA'\n",
        "    df.loc[df['employment_status']=='Unemployed', 'employment_occupation'] = 'NA'\n",
        "    df.loc[df['employment_status']=='Not in Labor Force', 'employment_industry'] = 'NA'\n",
        "    df.loc[df['employment_status']=='Not in Labor Force', 'employment_occupation'] = 'NA'\n",
        "    df.drop(['health_insurance'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzO3jWHo0SIW",
        "colab_type": "text"
      },
      "source": [
        "### Column engineering\n",
        "| feature | new level | new level | new level |\n",
        "| ------- | ------ | ------ | ------| \n",
        "| h1n1_concern | [(0,1) to become] 0 = less concern <br> | [(2,3) to become] 1 = more concern<br> | NONE | \n",
        "| household_adults (will be renamed: live_alone) | [(1,2,3) to become] 0 = does not live alone | [(0) to become] 1 = lives alone | NONE | \n",
        "| household_children (will be renamed: have_children) | [(0) to become] 0 = no children | [(1,2,3) to become] 1 = children | NONE | \n",
        "| employment_status | [(['Not in Labor Force', 'Unemployed']) to become] 0 = not working | [(['Employed']) to become] 1 = working | NONE |\n",
        "| education (will be renamed: college) | [(['< 12 Years', '12 Years']) to become] 0 = no college | [(['Some College', 'College Graduate']) to become] 1 = some college or more | NONE |\n",
        "| opinion_h1n1_vacc_effective | [(1,2) to become] 0 = not effective | [(3) to become] 1 = don't know | [(4,5) to become] 2 = effective | \n",
        "| opinion_seas_vacc_effective | [(1,2) to become] 0 = not effective | [(3) to become] 1 = don't know | [(4,5) to become] 2 = effective | \n",
        "|opinion_h1n1_sick_from_vacc | DROP 3 (DON'T KNOW) - too small, creates noise | [(1,2) to become] 0 = not worried | [(4,5) to become] 1 = worried |\n",
        "| opinion_seas_sick_from_vacc  | DROP 3 (DON'T KNOW) - too small, creates noise | [(1,2) to become] 0 = not worried | [(4,5) to become] 1 = worried |\n",
        "| opinion_h1n1_risk | DROP 3 (DON'T KNOW) - too small, creates noise | [(1,2) to become] 0 = low | [(4,5) to become] 1 = high |\n",
        "| opinion_seas_risk | DROP 3 (DON'T KNOW) - too small, creates noise | [(1,2) to become] 0 = low | [(4,5) to become] 1 = high |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqiDk2U82aQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## HELPER FUNCTIONS FOR TRANSFORMING COLUMNS (PER THE SPECIFICATIONS IN THE TABLE ABOVE)\n",
        "# rename columns\n",
        "def rename_col(df, old_name, new_name):\n",
        "  df.rename(columns={old_name : new_name}, inplace=True)\n",
        "  return\n",
        "\n",
        "# opinion_XXX_vacc_effective\n",
        "def opinion_vaccEffective_transformer(df, h1n1_or_szn):\n",
        "  colname = 'opinion_' + h1n1_or_szn + '_vacc_effective'\n",
        "  # extract indices to transform\n",
        "  from12_to0_idx = np.where(df[colname]<=2.0)[0].tolist()\n",
        "  from3_to1_idx = np.where(df[colname]==3.0)[0].tolist()\n",
        "  from45_to2_idx = np.where(df[colname]>=4.0)[0].tolist()\n",
        "  # transform\n",
        "  df.loc[from12_to0_idx, colname] = df.loc[from12_to0_idx, colname].replace(1.0, 0.0).replace(2.0, 0.0)\n",
        "  df.loc[from3_to1_idx, colname] = df.loc[from3_to1_idx, colname].replace(3.0, 1.0)\n",
        "  df.loc[from45_to2_idx, colname] = df.loc[from45_to2_idx, colname].replace(4.0, 2.0).replace(5.0, 2.0)\n",
        "  return\n",
        "\n",
        "# opinion_XXX_sick_from_vacc anad opinion_XXX_risk\n",
        "def opinion_XXX_transformer(df, h1n1_or_szn, vacc_or_risk):\n",
        "  if vacc_or_risk == 'vacc':\n",
        "      colname = 'opinion_' + h1n1_or_szn + '_sick_from_vacc'\n",
        "  elif vacc_or_risk == 'risk':\n",
        "    colname = 'opinion_' + h1n1_or_szn + '_risk'\n",
        "  # drop 3 ('don't know')\n",
        "  drop = np.where(df[colname]==3.0)[0].tolist()\n",
        "  df.drop(drop, inplace=True)\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # extract indices to transform\n",
        "  from12_to0_idx = np.where(df[colname]<=2.0)[0].tolist()\n",
        "  from45_to1_idx = np.where(df[colname]>=4.0)[0].tolist()\n",
        "  # transform\n",
        "  df.loc[from12_to0_idx, colname] = df.loc[from12_to0_idx, colname].replace(1.0, 0.0).replace(2.0, 0.0)\n",
        "  df.loc[from45_to1_idx, colname] = df.loc[from45_to1_idx, colname].replace(4.0, 1.0).replace(5.0, 1.0)\n",
        "  return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6Pfe19G-KkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## AGGREGATING ALL HELPER FUNCTIONS INTO ONE AND IMPLEMENTING THEM\n",
        "## TRANSFORMING THE REST OF THE COLUMNS \n",
        "\n",
        "def columnTransformer(df):\n",
        "  # h1n1_concern\n",
        "  # extract indices to transform\n",
        "  from01_to0_idx = np.where(df['h1n1_concern']<=1.0)[0].tolist()\n",
        "  from23_to1_idx = np.where(df['h1n1_concern']>=2.0)[0].tolist()\n",
        "  # transform\n",
        "  df.loc[from01_to0_idx, 'h1n1_concern'] = df.loc[from01_to0_idx, 'h1n1_concern'].replace(1.0, 0.0)\n",
        "  df.loc[from23_to1_idx, 'h1n1_concern'] = df.loc[from23_to1_idx, 'h1n1_concern'].replace(2.0, 1.0).replace(3.0,1.0)\n",
        "\n",
        "  # household_adults\n",
        "  rename_col(df,'household_adults', 'lives_alone')\n",
        "  # extract indices to transform\n",
        "  from123_to0_idx = np.where(df['lives_alone']>=1.0)[0].tolist()\n",
        "  from0_to1_idx = np.where(df['lives_alone']==0.0)[0].tolist()\n",
        "  # transform\n",
        "  df.loc[from123_to0_idx, 'lives_alone'] = df.loc[from123_to0_idx, 'lives_alone'].replace(1.0, 0.0).replace(2.0, 0.0).replace(3.0,0.0)\n",
        "  df.loc[from0_to1_idx, 'lives_alone'] = df.loc[from0_to1_idx, 'lives_alone'].replace(0.0, 1.0)\n",
        "\n",
        "  # household_children\n",
        "  rename_col(df,'household_children', 'have_children')\n",
        "  # extract indices to transform\n",
        "  from123_to1_idx = np.where(df['have_children']>=1.0)[0].tolist()\n",
        "  # transform\n",
        "  df.loc[from123_to1_idx, 'have_children'] = df.loc[from123_to1_idx, 'have_children'].replace(1.0, 1.0).replace(2.0, 1.0).replace(3.0,1.0)\n",
        "\n",
        "  # employment_status\n",
        "  # extract indices to transform\n",
        "  notEmp_idx = np.where(df['employment_status']!='Employed')[0].tolist()\n",
        "  emp_idx = np.where(df['employment_status']=='Employed')[0].tolist()\n",
        "  # transform\n",
        "  df.loc[notEmp_idx, 'employment_status'] = df.loc[notEmp_idx, \n",
        "                                                          'employment_status'].replace('Not in Labor Force', 0.0).replace('Unemployed', 0.0)\n",
        "  df.loc[emp_idx, 'employment_status'] = df.loc[emp_idx, 'employment_status'].replace('Employed', 1.0)\n",
        "\n",
        "  # education\n",
        "  # extract indices to transform\n",
        "  hs_idx = np.where((df['education']!='Some College') & (df['education']!='College Graduate'))[0].tolist()\n",
        "  college_idx = np.where((df['education']!='< 12 Years') & (df['education']!='12 Years'))[0].tolist()\n",
        "  # transform\n",
        "  df.loc[hs_idx, 'education'] = df.loc[hs_idx,'education'].replace('< 12 Years', 0.0).replace('12 Years', 0.0)\n",
        "  df.loc[college_idx, 'education'] = df.loc[college_idx,'education'].replace('Some College', 1.0).replace('College Graduate', 1.0)\n",
        "\n",
        "  opinion_vaccEffective_transformer(df, 'h1n1')\n",
        "  opinion_vaccEffective_transformer(df, 'seas') \n",
        "  opinion_XXX_transformer(df, 'h1n1', 'vacc')\n",
        "  opinion_XXX_transformer(df, 'seas', 'vacc') \n",
        "  opinion_XXX_transformer(df, 'h1n1', 'risk')\n",
        "  opinion_XXX_transformer(df, 'seas', 'risk') \n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IDL14RGaimox"
      },
      "source": [
        "### Encoding features\n",
        "| Feature type | Feature description | Associated features | Action |\n",
        "| ------------ | ------------------- | ------------------- | ------ |\n",
        "| binary | 2 levels [0,1] - <br> described as no/yes, low/high, etc | - 'h1n1_concern' <br> - 'behavioral_avoidance' <br> - 'behavioral_wash_hands' <br> - 'behavioral_large_gatherings' <br>- 'behavioral_outside_home' <br> - 'behavioral_touch_face' <br> - 'doctor_hecc_h1n1' <br> - 'doctor_recc_seasonal' <br> - 'chronic_med_condition' <br> - 'health_worker' <br>- 'opinion_h1n1_risk' <br>- 'opinion_h1n1_sick_from_vacc' <br>- 'opinion_seas_risk' <br>- 'opinion_seas_sick_from_vacc' <br> - employment_status <br>- 'live_alone' <br> - 'have_children' <br>- 'education' <br> - 'behavioral_antiviral_meds' <br> - 'behavioral_face_mask' <br> - 'child_under_6_months' | No action - 2 levels each (0,1) |\n",
        "| multiple | >2 levels [0,1,2,n...] | - 'opinion_h1n1_vacc_effective' <br>- 'opinion_seas_vacc_effective' <br> - 'h1n1_knowledge' | No action - 3 levels each | \n",
        "| text | levels are strings | - 'age_group' <br>- 'sex'<br>- 'income_poverty'<br>- 'rent_or_own'<br>- 'hhs_geo_region'<br>- 'census_msa'<br>- 'race'<br>- 'employment_industry'<br>- 'employment_occupation' <br>- marital_status | - census_msa and income_poverty to be manually ordinally encoded <br> - age_group to be ordinally encoded <br> - hhs_geo_region, race, employment_industry, employment_occupation <br>&nbsp; to be feature hash encoded OR binary encoded <br> - sex, marital_status, and rent_or_own to be one hot encoded (drop_first=True) |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSZDesrcKO9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# label encoding low cardinality (non-binary) text columns\n",
        "\n",
        "def manualEncoder(df):  \n",
        "  cols = df.columns\n",
        "  # manually encoding census_msa and income_poverty because they cannot be properly sorted \n",
        "  df['income_poverty'].replace('Below Poverty', 0, inplace=True)\n",
        "  df['income_poverty'].replace('<= $75,000, Above Poverty', 1, inplace=True)\n",
        "  df['income_poverty'].replace('> $75,000', 2, inplace=True)\n",
        "\n",
        "  df['census_msa'].replace('Non-MSA', 0, inplace=True)\n",
        "  df['census_msa'].replace('MSA, Not Principle  City', 1, inplace=True)\n",
        "  df['census_msa'].replace('MSA, Principle City', 2, inplace=True)\n",
        "\n",
        "  # ordinal encoding - age_group\n",
        "  df['age_group'], originalAges = pd.factorize(df['age_group'], sort=True)\n",
        "  df.columns = cols\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "  return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3MNPBz0Ay3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# binary encoding high cardinality text columns \n",
        "def binaryEncoder(train_df, test_df, binaryEncode_test=True):\n",
        "  feats = ['hhs_geo_region', 'race', 'employment_industry', 'employment_occupation']\n",
        "  transformed = pd.DataFrame()\n",
        "  transformed2 = pd.DataFrame()\n",
        "\n",
        "  for f in feats:\n",
        "    transformed_names = ['region_', 'race_', 'industry_', 'occup_']\n",
        "    lb = LabelBinarizer()\n",
        "\n",
        "    # train set\n",
        "    fit = lb.fit_transform(train_df[f])\n",
        "    transformed_feat = pd.DataFrame(fit, columns=[transformed_names[feats.index(f)]+str(col) for col in list(train_df[f].unique())])\n",
        "    transformed = pd.concat([transformed, transformed_feat], axis=1)\n",
        "    train_df.drop(f, axis=1, inplace=True)\n",
        "\n",
        "    if binaryEncode_test == True:\n",
        "      # test set\n",
        "      fit_testdata = lb.transform(test_df[f])\n",
        "      transformed_feat2 = pd.DataFrame(fit_testdata, columns=[transformed_names[feats.index(f)]+str(col) for col \n",
        "                                                                        in list(test_df[f].unique())])\n",
        "      transformed2 = pd.concat([transformed2, transformed_feat2], axis=1)\n",
        "      test_df.drop([f], axis=1, inplace=True)\n",
        "  \n",
        "  train_df = pd.concat([train_df, transformed], axis=1)\n",
        "  test_df = pd.concat([test_df, transformed2], axis=1)\n",
        "  return train_df, test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jds7gct_AAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature hash encoding high cardinality text columns \n",
        "\n",
        "def featureHashEncoder(train_df, test_df, featurehashEncode_test):\n",
        "  feats = ['hhs_geo_region', 'race', 'employment_industry', 'employment_occupation']\n",
        "  transformed = pd.DataFrame()\n",
        "  transformed2 = pd.DataFrame()\n",
        "\n",
        "  for f in feats:\n",
        "    transformed_names = ['region_', 'race_', 'industry_', 'occup_']\n",
        "    fh = FeatureHasher(n_features=len(train_df[f].unique()), input_type='string')\n",
        "\n",
        "    # train set\n",
        "    fit = fh.fit_transform(train_df[f])\n",
        "    transformed_feat = pd.DataFrame(fit.toarray(), columns=[transformed_names[feats.index(f)]+str(col) for col in list(train_df[f].unique())])\n",
        "    transformed = pd.concat([transformed, transformed_feat], axis=1)\n",
        "    train_df.drop(f, axis=1, inplace=True)\n",
        "\n",
        "    if featurehashEncode_test == True:\n",
        "      # test set\n",
        "      fit_testdata = fh.transform(test_df[f])\n",
        "      transformed_feat2 = pd.DataFrame(fit_testdata.toarray(), columns=[transformed_names[feats.index(f)]+str(col) for col \n",
        "                                                                        in list(test_df[f].unique())])\n",
        "      transformed2 = pd.concat([transformed2, transformed_feat2], axis=1)\n",
        "      test_df.drop([f], axis=1, inplace=True)\n",
        "  \n",
        "  train_df = pd.concat([train_df, transformed], axis=1)\n",
        "  test_df = pd.concat([test_df, transformed2], axis=1)\n",
        "  return train_df, test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAevGkv8_ERq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encoding binary text columns\n",
        "\n",
        "def onehotEncoder(df):\n",
        "  df['sex'] = pd.get_dummies(df['sex'], prefix='sex', prefix_sep='_', drop_first=True) # returns MALE\n",
        "  df['rent_or_own'] = pd.get_dummies(df['rent_or_own'], prefix='rentVown', prefix_sep='_', drop_first=True) # returns RENT\n",
        "  df['marital_status'] = pd.get_dummies(df['marital_status'], prefix='married', prefix_sep='_', drop_first=True) # returns NOT MARRIED\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCra4DHq_H63",
        "colab_type": "text"
      },
      "source": [
        "aggregate the above functions into one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NXCKypy_G07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AGGREGATOR\n",
        "def encoder(train_df, test_df, binary_or_featurehash, encode_test=True):\n",
        "  manualEncoder(train_df)\n",
        "  manualEncoder(test_df)\n",
        "  if binary_or_featurehash=='binary':\n",
        "    train_df, test_df = binaryEncoder(train_df, test_df, encode_test)\n",
        "  elif binary_or_featurehash=='featurehash':  \n",
        "    train_df, test_df = featureHashEncoder(train_df, test_df, encode_test) \n",
        "  onehotEncoder(train_df)\n",
        "  onehotEncoder(test_df)\n",
        "  return train_df, test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfuJ8XSpiHgL",
        "colab_type": "text"
      },
      "source": [
        "## Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q58L9Tu5cGs",
        "colab_type": "text"
      },
      "source": [
        "#### Feature selection using VIF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZKnIKujUDF-K",
        "colab": {}
      },
      "source": [
        "# remove multicolinear features - not necessary for boosted trees, but good practice to do so \n",
        "def checkVIF(df):\n",
        "  vif = pd.DataFrame()\n",
        "  vif[\"VIF Factor\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
        "  vif[\"features\"] = df.columns\n",
        "  return vif"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9LtvS57aNqz",
        "colab_type": "text"
      },
      "source": [
        "#### Feature selection using correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9D0V4o7EVYX",
        "colab_type": "text"
      },
      "source": [
        "##### Correlations using Cramer's V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re36sA10yprS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cramersV(df):\n",
        "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
        "        uses correction from Bergsma and Wicher, \n",
        "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
        "    \"\"\"\n",
        "    cols = df.columns\n",
        "    \n",
        "    combos = itertools.combinations_with_replacement(cols, 2)\n",
        "    cramers = []\n",
        "    for c in combos: \n",
        "        # create contingency table \n",
        "        contingencyTbl = pd.crosstab(df[c[0]], df[c[1]])\n",
        "        \n",
        "        # cramer's v\n",
        "        chi2 = stats.chi2_contingency(contingencyTbl)[0]\n",
        "        n = contingencyTbl.sum().sum()\n",
        "        phi2 = chi2/n\n",
        "        r,k = contingencyTbl.shape\n",
        "        phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
        "        rcorr = r - ((r-1)**2)/(n-1)\n",
        "        kcorr = k - ((k-1)**2)/(n-1)\n",
        "        cramersv = np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n",
        "        cramers.append([c[0], c[1], cramersv])\n",
        "        \n",
        "    mat = pd.DataFrame(np.zeros((len(cols), len(cols))))\n",
        "    mat.index = cols \n",
        "    mat.columns = cols\n",
        "    for c in cramers: \n",
        "        mat.loc[c[0],c[1]] = round(c[2],2)\n",
        "        mat.loc[c[1],c[0]] = round(c[2],2)\n",
        "    return mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYrbCEGf_M6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plotting the correlations\n",
        "def corrPlot(corrMat, name):\n",
        "    sns.set(font_scale=2)\n",
        "    fig, ax = plt.subplots(figsize=(35,39))\n",
        "    heatmap = sns.heatmap(corrMat, annot=True, annot_kws={\"size\": 20}, \n",
        "                        linewidths=0.75, cmap='Blues', cbar=False, ax=ax)\n",
        "    heatmap.set_title('{} CORRELATIONS'.format(name.upper()), fontsize=48)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXTtCGMW_akw",
        "colab_type": "text"
      },
      "source": [
        "#####Correlation Analysis \n",
        "returns pairs of features and chooses a feature in each pair to drop based on mean correlations with other variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzvsCdqSeLMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## HELPER FUNCTIONS FOR ANALYZING THE CORRELATIONS\n",
        "\n",
        "# find highly correlated features (indices and features)\n",
        "def find_corrFeatures(corrMat, corrThresh):\n",
        "  ## indices\n",
        "  correlated_idx = []\n",
        "  for row_idx in range(len(corrMat.values)):\n",
        "    for col_idx in range(len(corrMat.values[row_idx])):\n",
        "      if (corrMat.values[row_idx, col_idx] > corrThresh and row_idx!=col_idx):\n",
        "        if list(set([row_idx,col_idx])) not in correlated_idx:\n",
        "          correlated_idx.append(list(set([row_idx,col_idx])))\n",
        "\n",
        "  ## features\n",
        "  f1 =[]\n",
        "  f2 = []\n",
        "  for c in range(len(correlated_idx)):\n",
        "    f1.append(correlated_idx[c][0])\n",
        "    f2.append(correlated_idx[c][1])\n",
        "  return f1, f2\n",
        "\n",
        "# find correlations for highly correlated features\n",
        "def find_corrs(corrMat, f1, f2):\n",
        "  # rows and columns\n",
        "  featureList1 = list(corrMat.index)\n",
        "  featureList2 = list(corrMat.columns)\n",
        "\n",
        "  # find the correlations for the highly correlated features\n",
        "  corr_feats = pd.concat([pd.Series([featureList1[f] for f in f1]), pd.Series([featureList2[f] for f in f2])], axis=1)\n",
        "  bad_corrs = [corrMat.loc[corr_feats.iloc[c,0], corr_feats.iloc[c,1]] for c in range(len(corr_feats))]\n",
        "  problems_df = pd.concat([corr_feats.iloc[:,0], corr_feats.iloc[:,1], pd.Series(bad_corrs)], axis=1)\n",
        "  problems_df.columns = ['feature1', 'feature2', 'cramerv']\n",
        "  return problems_df, bad_corrs\n",
        "\n",
        "# Find mean correlation of each of the highly correlated features with other variables (each row)\n",
        "def find_meanCorrs(corrMat, problems_df, bad_corrs):\n",
        "  try:\n",
        "    corrMat.drop(['h1n1_vaccine', 'seasonal_vaccine'],axis=1, inplace=True)\n",
        "    corrMat.drop(['h1n1_vaccine', 'seasonal_vaccine'], inplace=True)\n",
        "  except KeyError:\n",
        "    pass \n",
        "\n",
        "  meancorrs1 = [round(np.mean(corrMat.loc[l,:].drop(l)),2) for l in list(problems_df['feature1'])]\n",
        "  meancorrs2 = [round(np.mean(corrMat.loc[:,l].drop(l)),2) for l in list(problems_df['feature2'])]\n",
        "\n",
        "  zipped_corrs = list(zip(meancorrs1,meancorrs2))\n",
        "  max_meancorrs = [max(corrs) for corrs in zipped_corrs]\n",
        "  zipped_meancorrs = list(zip(max_meancorrs, zipped_corrs))\n",
        "  return zipped_meancorrs\n",
        "\n",
        "# Return the features in each row with the highest mean correlation with other variables\n",
        "def find_highCorrFeatures(zipped_meancorrs, problems_df):\n",
        "  to_remove = []\n",
        "  for col_idx in range(len(zipped_meancorrs)):\n",
        "    biggest = zipped_meancorrs[col_idx][0]\n",
        "    both = list(zipped_meancorrs[col_idx][1])\n",
        "    for row_idx in range(len(both)): \n",
        "        if both[row_idx] == biggest:\n",
        "          to_remove.append([col_idx,row_idx])\n",
        "  corr_TOREMOVE = pd.DataFrame([[bad[0], problems_df.iloc[bad[0], bad[1]]] for bad in to_remove], columns = ['row', 'feature_name'])\n",
        "  return corr_TOREMOVE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyFbyZwuAVBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## AGGREGATING INFO GATHERED USING HELPER FUNCTIONS AND RETURN NEATLY\n",
        "def organize_corrAnalysis(corr_TOREMOVE, problems_df):\n",
        "  # creating the dataframe\n",
        "  feature_todrop = []\n",
        "  for idx in range(len(problems_df)):\n",
        "    droppable = corr_TOREMOVE[corr_TOREMOVE['row']==idx]\n",
        "    if len(droppable)==2:\n",
        "      feature_todrop.append('both')\n",
        "    else: \n",
        "      feature_todrop = feature_todrop + droppable['feature_name'].values.tolist()\n",
        "\n",
        "  corrAnalysis_df = pd.concat([problems_df.iloc[:,:2], pd.Series(feature_todrop), problems_df.iloc[:,-1]], axis=1)\n",
        "  corrAnalysis_df.columns = ['compared_feature1', 'compared_feature2', 'feature_to_drop', 'cramersV_for_featuretoDrop'] \n",
        "\n",
        "  # aggregate final list of features to be dropped\n",
        "  features_toDrop = []\n",
        "  for idx in range(len(corrAnalysis_df)):\n",
        "    if corrAnalysis_df.loc[idx,'feature_to_drop']=='both':\n",
        "      features_toDrop.append(corrAnalysis_df.loc[idx, 'compared_feature1'])\n",
        "      features_toDrop.append(corrAnalysis_df.loc[idx,'compared_feature2'])\n",
        "    else:\n",
        "      features_toDrop.append(corrAnalysis_df.loc[idx,'feature_to_drop'])\n",
        "  features_toDrop = list(set(features_toDrop))\n",
        "  return corrAnalysis_df, features_toDrop\n",
        "\n",
        "# AGGREGATING EVERYTHING UP THERE INTO ONE FUNCTION \n",
        "def corrAnalysis(corrMat, corrThresh):\n",
        "  features1, features2 = find_corrFeatures(corrMat, corrThresh)\n",
        "  problem_df, problem_corrs = find_corrs(corrMat, features1, features2)\n",
        "  zipped = find_meanCorrs(corrMat, problem_df, problem_corrs)\n",
        "  bad_features = find_highCorrFeatures(zipped, problem_df)\n",
        "  analysis_df, to_drop = organize_corrAnalysis(bad_features, problem_df)\n",
        "  return analysis_df, to_drop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc-hKbx-EbTp",
        "colab_type": "text"
      },
      "source": [
        "#### Remove unbalanced columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocs28fC9hnvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# style function\n",
        "def color_lowSamples(val):\n",
        "    \"\"\"\n",
        "    Takes a scalar and returns a string with\n",
        "    the css property `'color: red'` for negative\n",
        "    strings, black otherwise.\n",
        "    \"\"\"\n",
        "    color = 'red' if val <= 15 else 'black'\n",
        "    return 'color: %s' % color"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1nFF_WGBGbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# value counts for each level of each category \n",
        "def colVals(df, plots=1):\n",
        "  sns.set(font_scale=1)\n",
        "\n",
        "  cols = list(df.columns)\n",
        "  percents = []\n",
        "\n",
        "  for col in cols:\n",
        "    percent_perLevel = round(df[col].value_counts()/len(df[col])*100,2)\n",
        "    percents.append(percent_perLevel)\n",
        "\n",
        "    if plots == 1:\n",
        "      colVals = pd.DataFrame(df[col].value_counts())\n",
        "      ax = sns.countplot(df[col])\n",
        "      plt.xticks(rotation=90)\n",
        "      plt.xlabel(col.replace('_', ' '))\n",
        "      plt.ylabel('number of participants')\n",
        "\n",
        "      plt.show()\n",
        "\n",
        "    elif plots == 0:\n",
        "      pass\n",
        "\n",
        "    else:\n",
        "      print('invalid plots value')\n",
        "  return percents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq79gD-eaG3e",
        "colab_type": "text"
      },
      "source": [
        "#### Feature importances using random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd1mUUzi5JRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scale(features, labels):\n",
        "  fcols = features.columns\n",
        "  mm = MinMaxScaler()\n",
        "  features_transformed = pd.DataFrame(mm.fit_transform(features), columns=fcols)\n",
        "  features_transformed = pd.DataFrame(features_transformed, columns = fcols)\n",
        "  labels_transformed = pd.Series(labels, name = labels.name)\n",
        "  return features_transformed, labels_transformed\n",
        "\n",
        "def balance(features, labels):\n",
        "  fcols = features.columns\n",
        "  sm = SMOTE(random_state=0)\n",
        "  features_transformed, labels_transformed  = sm.fit_resample(features, labels)\n",
        "  features_transformed = pd.DataFrame(features_transformed, columns = fcols)\n",
        "  labels_transformed = pd.Series(labels_transformed, name = labels.name)\n",
        "  return features_transformed, labels_transformed"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}